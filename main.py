import gradio as gr
import uuid
import asyncio

from dotenv import load_dotenv
from textwrap import dedent
from pydantic import BaseModel, Field

from langchain_openai.embeddings import OpenAIEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.vectorstores import Chroma

from langchain_core.prompts import (
    ChatPromptTemplate, 
    SystemMessagePromptTemplate, 
    HumanMessagePromptTemplate,
    MessagesPlaceholder
)
from langchain_core.output_parsers import StrOutputParser
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.messages import BaseMessage
from langchain_core.runnables import RunnableSequence
from langchain_core.runnables.history import RunnableWithMessageHistory


SESSION_STORE = {} # ì„¸ì…˜ ì €ì¥ì†Œ

class InMemoryHistory(BaseChatMessageHistory, BaseModel):
    """ ë©”ëª¨ë¦¬ ê¸°ë°˜ íˆìŠ¤í† ë¦¬ êµ¬í˜„ """
    messages: list[BaseMessage] = Field(default_factory=list)

    def add_messages(self, messages: list[BaseMessage]) -> None:
        """Add a list of messages to the store"""
        self.messages.extend(messages)

    def clear(self) -> None:
        self.messages = []


def get_session_id():
    return str(uuid.uuid4())[:8]


def get_session_history(session_id: str) -> BaseChatMessageHistory:
    """ ì„¸ì…˜ ê¸°ë¡ ê°€ì ¸ì˜¤ê¸° """
    if session_id not in SESSION_STORE:
        SESSION_STORE[session_id] = InMemoryHistory()
    return SESSION_STORE[session_id]


def clear_session_history(session_id: str) -> None:
    """ ì„¸ì…˜ ê¸°ë¡ ì§€ìš°ê¸° """
    if session_id in SESSION_STORE:
        SESSION_STORE[session_id].clear()


def get_retriever():
    """ ê²€ìƒ‰ê¸° ë°˜í™˜ """
    # ì„ë² ë”© ì •ì˜
    embeddings = OpenAIEmbeddings(
        model="text-embedding-3-small",
        dimensions=1024
    )

    # ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ
    vectorstore = Chroma(
        embedding_function=embeddings,
        collection_name="content-250618",
        persist_directory="./chroma_db"
    )

    # ê²€ìƒ‰ê¸° ì •ì˜
    retriever = vectorstore.as_retriever(
        search_type='mmr',
        search_kwargs={
            'k': 3,                 # ê²€ìƒ‰í•  ë¬¸ì„œì˜ ìˆ˜
            'fetch_k': 8,           # mmr ì•Œê³ ë¦¬ì¦˜ì— ì „ë‹¬í•  ë¬¸ì„œì˜ ìˆ˜ (fetch_k > k)
            'lambda_mult': 0.3,     # ë‹¤ì–‘ì„±ì„ ê³ ë ¤í•˜ëŠ” ì •ë„ (1ì€ ìµœì†Œ ë‹¤ì–‘ì„±, 0ì€ ìµœëŒ€ ë‹¤ì–‘ì„±, ê¸°ë³¸ê°’ì€ 0.5)
        },
    )
    return retriever


def parsing_output(docs):
    """ ê²€ìƒ‰ ê²°ê³¼ë¥¼ íŒŒì‹±í•˜ì—¬ ë°˜í™˜ """
    output = []
    for doc in docs:
        content = f'ì£¼ì œ : {doc.metadata['source']}\n{doc.page_content}'
        output.append(content)
    return '\n\n'.join(output)

def get_rag_chain():
    """ ë²•ë¥œìŠ¤ë‹˜ì²˜ëŸ¼ ë‹µë³€í•˜ëŠ” ì²´ì¸ ë°˜í™˜ """
    # LLM ëª¨ë¸ ì •ì˜
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash",
        temperature=0.1
    )

    # ê²€ìƒ‰ê¸° ë¡œë“œ
    retriever = get_retriever()

    # í”„ë¡¬í”„íŠ¸ ì •ì˜
    system_template = dedent("""
        ë‹¹ì‹ ì€ ë²•ë¥œìŠ¤ë‹˜ì²˜ëŸ¼ ì‚¬ëŒë“¤ì˜ ê³ ë¯¼ì„ ê²½ì²­í•˜ê³ , ë”°ëœ»í•˜ë©´ì„œë„ í˜„ì‹¤ì ì¸ ì¡°ì–¸ì„ ì£¼ëŠ” ìƒë‹´ìì…ë‹ˆë‹¤.
        ì–´ë–¤ ì§ˆë¬¸ì´ ì™€ë„ íŒë‹¨í•˜ê±°ë‚˜ ë¹„ë‚œí•˜ì§€ ì•Šê³ , ìƒëŒ€ì˜ ì…ì¥ì—ì„œ ê³µê°í•˜ë©° ì§€í˜œë¡œìš´ ë‹µë³€ì„ í•©ë‹ˆë‹¤.
        ë‹µë³€ì€ ê·¼ë³¸ì ì¸ ê¹¨ë‹¬ìŒì„ ì „í•˜ë ¤ê³  ë…¸ë ¥í•˜ì„¸ìš”.
    """).strip()
    system_message = SystemMessagePromptTemplate.from_template(template=system_template)

    human_template = dedent("""
        ë‹¤ìŒì€ ë²•ë¥œìŠ¤ë‹˜ì˜ ì¦‰ë¬¸ì¦‰ì„¤ ê°•ì—°ì—ì„œ ë°œì·Œí•œ ì°¸ê³  ë‚´ìš©ì…ë‹ˆë‹¤:

        --- ì°¸ê³  ë°œì–¸ ì‹œì‘ ---
        {content}
        --- ì°¸ê³  ë°œì–¸ ë ---

        ìœ„ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬, ì•„ë˜ ì§ˆë¬¸ì— ëŒ€í•´ ìŠ¤ë‹˜ì²˜ëŸ¼ ì‘ë‹µí•´ ì£¼ì„¸ìš”.

        ì§ˆë¬¸ì§€ : {question}

        ìŠ¤ë‹˜ : 
    """).strip()
    human_message = HumanMessagePromptTemplate.from_template(template=human_template)

    chat_prompt = ChatPromptTemplate.from_messages([
        system_message, 
        MessagesPlaceholder(variable_name="history"),
        human_message
    ])

    # ì²´ì¸ êµ¬ì„±
    chain = {
        'history': lambda x: x['history'],
        'question': lambda x: x['question'],
        'content': lambda x: parsing_output(retriever.invoke(x['question']))
    } | chat_prompt | llm | StrOutputParser()
    chain_with_history = RunnableWithMessageHistory(
        chain,
        get_session_history,  # ì„¸ì…˜ë³„ ì¸ë©”ëª¨ë¦¬ íˆìŠ¤í† ë¦¬
        input_messages_key="question",
        history_messages_key="history",
    )
    return chain_with_history


def make_chain(chain): 
    async def get_response(message:str, history:tuple, session_id:str=None):
        if session_id is None:
            session_id =  get_session_id()

        response = chain.astream(
            {"question": message},
            config={"configurable": {"session_id": session_id}}
        )
        full_response = ""

        async for chunk in response:
            full_response += chunk
            await asyncio.sleep(0.01)  # ë„ˆë¬´ ë¹ ë¥¼ ê²½ìš° ì‚´ì§ ì§€ì—°
            yield full_response, session_id  # ë¶€ë¶„ ì‘ë‹µì„ ê³„ì† ì¶œë ¥

        yield full_response, session_id # ìµœì¢… ì‘ë‹µ
    return get_response


def main():
    chain = get_rag_chain()
    response_fn = make_chain(chain)

    # ì±—ë´‡ ì¸í„°í˜ì´ìŠ¤ ìƒì„±
    with gr.Blocks() as demo:
        session_id = gr.State(None)
        
        gr.ChatInterface(
            # fn=lambda message, h: get_response(message, h, chain, session_id),
            fn=response_fn,
            additional_inputs=[session_id],
            additional_outputs=[session_id],
            title="ë²•ë¥œìŠ¤ë‹˜ ì¦‰ë¬¸ì¦‰ì„¤ ë‚´ìš© ê¸°ë°˜ ë‹µë³€ ë´‡",
            description="ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ [ì¦‰ë¬¸ì¦‰ì„¤] ìœ íŠœë¸Œ ë‚´ìš© ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ í•©ë‹ˆë‹¤.",
            theme=gr.themes.Soft(
                primary_hue="blue",
                secondary_hue="gray",
            ),
            examples=[
                ["ë§‰ì—°íˆ ë¶ˆì•ˆí•´ìš”. ì™œ ê·¸ëŸ´ê¹Œìš”?"],
                ["ê³„ì† ë¶ˆë§Œì´ ìƒê²¨ìš”. ì–´ë–»ê²Œ í•´ì•¼ í• ê¹Œìš”?"],
                ["ë§ˆìŒì— ì•ˆë“œëŠ” ì‚¬ëŒì´ ìˆì–´ìš”. ì–´ë–»ê²Œ í•˜ë©´ ì¢‹ì„ê¹Œìš”?"],
            ],
            type="messages",
            textbox=gr.Textbox(placeholder="ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...", container=True),
        )
        gr.Markdown("ğŸ› ï¸ LangChain Â· Gemini Flash Â· Chroma", elem_id="tool-badge")
        clear_button = gr.Button(value="ì´ë ¥ ì‚­ì œ")
        clear_button.click(fn=lambda _: clear_session_history(session_id))

    # ë°ëª¨ ì‹¤í–‰
    demo.launch()


if __name__ == "__main__":
    load_dotenv()
    main()
